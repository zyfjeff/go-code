6.824 2016 课程1: 介绍
6.824 2016 Lecture 1: Introduction
6.824: 分布式系统工程
6.824: Distributed Systems Engineering

什么是分布式系统?
多计算机协作，DNS,P2P的文件共享，大型数据 MapReduce，以及很多的关键性基础架构都是分布式的
What is a distributed system?
  multiple cooperating computers
  DNS, P2P file sharing, big databases, MapReduce, &c
  lots of critical infrastructure is distributed!

为什么要分布式?
去连接物理上分离的实体
去通过隔离获得安全性
去通过复制来容错
去通过并行的CPU/mem/disk/net来达到横向扩展
Why distribute?
  to connect physically separate entities
  to achieve security via isolation
  to tolerate faults via replication
  to scale up throughput via parallel CPUs/mem/disk/net

但是:
    复杂:许多并发的组件，必须处理好部分组件失败的情况， 很难去了解性能的瓶颈
But:
  complex: many concurrent parts
  must cope with partial failure
  tricky to realize performance potential

为什么要开这门课程?
    有趣 -- 问题很难，没有很明显的解决方案，
    被用于真实的系统上 -- 因大型网站的崛起而驱动
    很活跃的研究领域 -- 很多改进的空间 + 大的尚未解决的问题

Why take this course?
  interesting -- hard problems, non-obvious solutions
  used by real systems -- driven by the rise of big Web sites
  active research area -- lots of progress + big unsolved problems
  hands-on -- you'll build serious systems in the labs

课程结构
COURSE STRUCTURE

http://pdos.csail.mit.edu/6.824

Course staff:
  Robert Morris, lecturer
  Frans Kaashoek, lecturer
  Steven Allen, TA
  Stephanie Wang, TA
  Jon Gjengset, TA
  Daniel Ziegler, TA

课程组成:
    课件
    读物
    两个考试
    实验
    最终的项目

Course components:
  lectures
  readings
  two exams
  labs
  final project

课件主要是实验，大的ideas，还有paper
Lectures about big ideas, papers, and labs

读物: 研究paper作为学习案列
    请在上课前阅读paper，否则会很无聊，你没办法通过听课来学会
    每个paper，都有一些短小的问题需要你回答，你必须告诉我一些
    关于这个paper的一些问题，在晚上十点之前提交问题和答案

Readings: research papers as case studies
  please read papers before class
    otherwise boring, and you can't pick it up by listening
  each paper has a short question for you to answer
  and you must send us a question you have about the paper
  submit question&answer by 10pm the night before

Mid-term exam in class, and final exam

实验目标:
    深度理解一些重要的技术，分布式编程的经验
Lab goals:
  deeper understanding of some important techniques
  experience with distributed programming
  first lab is due a week from Friday

Lab 1: MapReduce
Lab 2: replication for fault-tolerance
Lab 3: fault-tolerant key/value store
Lab 4: sharded key/value store

Final project at the end, in groups of 2 or 3.
  You can think of a project and clear it with us.
  Or you can do a "default" project that we'll specify.

Lab grades depend on how many test cases you pass
  we give you the tests, so you know whether you'll do well
  careful: if it usually passes, but sometimes fails,
    chances are it will fail when we run it

Lab code review
  look at someone else's lab solution, send feedback
  perhaps learn about another approach

Debugging the labs can be time-consuming
  start early
  come to TA office hours
  ask questions on Piazza

MAIN TOPICS
这是一个关于基础架构的课程，常备应用程序所使用。
关于抽象，就是隐藏了分布式应用程序的复杂性
有三大类抽象:
    存储
    通信
    计算
若干个主题被反复提出来

This is a course about infrastructure, to be used by applications.
  About abstractions that hide complexity of distribution from applications.
  Three big kinds of abstraction:
    Storage.
    Communication.
    Computation.
  A couple of topics come up repeatedly.

主题: 实现
RPC,线程,并发控制
Topic: implementation
  RPC, threads, concurrency control.

主题: 性能
    期望: 可伸缩的吞吐量
        Nx台 服务器 -> 通过并行的CPU,磁盘,网络提高Nx倍吞吐量
        因此处理更高的负载只需要买更多的计算机
    扩展会变得越来越难:
        负载均衡，离散者问题()
        "很小"一部分非并行的部分
        隐藏共享资源,比如 网络

Topic: performance
  The dream: scalable throughput.
    Nx servers -> Nx total throughput via parallel CPU, disk, net.
    So handling more load only requires buying more computers.
  Scaling gets progressively harder:
    Load balance, stragglers.
    "Small" non-parallelizable parts.
    Hidden shared resources, e.g. network.

主题: 容错
    1000台服务器，复杂的网络 -> 总是会出现某些服务挂掉了
    我们希望在应用程序程序隐藏这些失败的情况
    我们通常希望用下面两个来度量:
        可用性 -- 我可以保持使用我的文件无论是否失败
        持久性 -- 当故障被修复了，我的文件可以继续使用
    思路: 副本服务器
     如果一个服务器crashes了，client可以使用其他的服务器
Topic: fault tolerance
  1000s of server, complex net -> always something broken
  We'd like to hide these failures from the application.
  We often want:
    Availability -- I can keep using my files despite failures
    Durability -- my data will come back to life when failures are repaired
  Big idea: replicated servers.
    If one server crashes, client can proceed using the other.

主题: 一致性
    基本目的是基础设施需要一个明确的行为
        比如: Get(k)得到的结果是来自与最近一次的put(k,v)
    完成良好的行为是很难的!
        Clients提交并发操作
        Servers crash在不恰当的时刻
        网络可能会让存活的服务器看起来是死的，产生脑裂的现象
    一致性和性能是敌人
        一致性需要通信,比如: 得到最后一次Put的结果
        系统的严格语义通常很慢
        快速的系统通过会让程序处理复杂的行为
Topic: consistency
  General-purpose infrastructure needs well-defined behavior.
    E.g. "Get(k) yields the value from the most recent Put(k,v)."
  Achieving good behavior is hard!
    Clients submit concurrent operations.
    Servers crash at awkward moments.
    Network may make live servers look dead; risk of "split brain".
  Consistency and performance are enemies.
    Consistency requires communication, e.g. to get latest Put().
    Systems with pleasing ("strict") semantics are often slow.
    Fast systems often make applications cope with complex ("relaxed") behavior.
  People have pursued many design points in this spectrum.

案列研究: MapReduce
CASE STUDY: MapReduce

让我们把MapReduce(MR)作为一个案列来学习
    MR是是6.824's课程主要主题中的一个很好的例子，它也是Lab 1需要关注的。

Let's talk about MapReduce (MR) as a case study
  MR is a good illustration of 6.824's main topics
  and is the focus of Lab 1

MapReduce 概述
内容: 在几千兆的数据集上的多个小时的计算
    比如: 分布式系统的爱好者通常不会去开发，实验分析爬取的web页面的结构的程序，因为这会非常的痛苦，例如需要处理失败
总体目标: 不需要专家级别的程序员就可以简单并且高效合理的解决巨大的数据处理的问题
    程序员只需要定义一个Map和一个Reduce函数即可，都是顺序执行的代码；通常是相当的简单。
    MR将程序员编写的函数运行在1000s台主机，并输入一个巨大的数据集，并屏蔽底层所有的分布式相关的细节

MapReduce overview
  context: multi-hour computations on multi-terabyte data-sets
    e.g. experimental analyses of structure of crawled web pages
    often not developed by distributed systems enthusiasts
    can be very painful, e.g. coping with failure
  overall goal: non-specialist programmers can easily solve giant
    data processing problems with reasonable efficiency.
  programmer defines Map and Reduce functions
    sequential code; often fairly simple
  MR runs the functions on 1000s of machines with huge inputs
    and hides all details of distribution
  
MapReduce的抽象视图
Abstract view of MapReduce
  input is divided into "splits"
  Input Map -> a,1 b,1 c,1
  Input Map ->     b,1
  Input Map -> a,1     c,1
                |   |   |
                    |   -> Reduce -> c,2
                    -----> Reduce -> b,2
  MR calls Map() on each split, produces set of k2,v2
    "intermediate" data
  MR gathers all intermediate v2's for a given k2,
    and passes them to a Reduce call
  final output is set of <k2,v3> pairs from Reduce()

Example: word count
  input is thousands of text files
  Map(k, v)
    split v into words
    for each word w
      emit(w, "1")
  Reduce(k, v)
    emit(len(v))

这个模型很容易去编程，它隐藏了很多令人不快的细节
This model is easy to program; it hides many painful details:
  concurrency -- same result as sequential execution
  starting s/w on servers
  data movement
  failures

This model scales well.
  Nx computers get you Nx Map() and Reduce() throughput.
    Map()s don't wait for each other or share data, can run in parallel.
    Same for Reduce()s.
    Up to a point...
  So you can get more throughput by buying more computers.
    Rather than special-purpose efficient parallelizations of each application.
    Computers are much cheaper than programmers!

What will be the limiting factor in performance?
  We care since that's the thing to optimize.
  CPU? memory? disk? network?
  They were limited by "network cross-section bandwidth".
    [diagram: map servers, network box, reduce servers]
    The network's total internal capacity.
    Often much smaller than sum of host network link speeds.
  Hard to build a network than can run 1000x faster than a single computer.
  So they cared about minimizing movement of data over the network.

What about fault tolerance?
  I.e. what if a server crashes during a MR job?
  Hiding failures is a huge part of ease of programming!
  Why not re-start the whole job from the beginning?
  MR re-runs just the failed Map()s and Reduce()s.
    They are pure functions -- they don't modify their inputs,
      they don't keep state, there's no shared memory, there's
      no map/map or reduce/reduce interaction.
    So re-execution is likely to yield the same output.
  The requirement for pure functions is a major limitation of
    MR compared to other parallel programming schemes.
    But it's critical to MR's simplicity.

More details (paper's Figure 1):
  master: gives tasks to workers; remembers where intermediate output is
  M input splits
  input stored in GFS, 3 copies of each split
  all computers run both GFS and MR workers
  many more input splits than workers
  master starts a Map task on each server
    hands out new tasks as old ones finish
  worker hashes Map output by key into R partitions, on local disk
  no Reduce calls until all Maps are finished
  master tells Reducers to fetch intermediate data partitions from Map workers
  Reduce workers write final output to GFS

How does detailed design help network performance?
  Map input is read from local disks, not over network.
  Intermediate data goes over network just once.
    Stored on local disk, not GFS.
  Intermediate data partitioned into files holding many keys.
    Big network transfers are more efficient.

How do they get good load balance?
  Critical to scaling -- otherwise Nx servers -> no gain.
  But time to process a split or partition isn't uniform.
    Different sizes and contents, and different server hardware.
  Solution: many more splits than workers.
    Master hands out new splits to workers who finish previous tasks.
    So no split is so big it dominates completion time (hopefully).
    So faster servers do more work than slower ones, finish abt the same time.

How does MR cope with worker crashes?
  * Map worker crashes:
    master re-runs, spreads tasks over other GFS replicas of input.
      even if worker had finished, since still need intermediate data on disk.
    some Reduce workers may already have read failed worker's intermediate data.
      here we depend on functional and deterministic Map()!
    how does the master know the worker crashed? (pings)
    master need not re-run Map if Reduces have fetched all intermediate data
      though then a Reduce crash would have to wait for Maps to re-run
  * Reduce worker crashes before producing output.
    master re-starts its tasks on another worker.
  * Reduce worker crashes in the middle of writing its output.
    GFS has atomic rename that prevents output from being visible until complete.
    so it's safe for the master to re-run the Reduce tasks somewhere else.

Other failures/problems:
  * What if the master accidentally starts *two* Map() workers on same input?
    it will tell Reduce workers about only one of them.
  * What if two Reduce() workers for the same partition of intermediate data?
    they will both try to write the same output file on GFS!
    atomic GFS rename will cause the second to finish to win.
  * What if a single worker is very slow -- a "straggler"?
    perhaps due to flakey hardware.
    master starts a second copy of last few tasks.
  * What if a worker computes incorrect output, due to broken h/w or s/w?
    too bad! MR assumes "fail-stop" CPUs and software.
  * What if the master crashes?

For what applications *doesn't* MapReduce work well?
  Not everything fits the map/shuffle/reduce pattern.
  Small data, since overheads are high. E.g. not web site back-end.
  Small updates to big data, e.g. add a few documents to a big index
  Unpredictable reads (neither Map nor Reduce can choose input)
  Multiple shuffles, e.g. page-rank (can use multiple MR but not very efficient)
  More flexible systems allow these, but more complex model.

Conclusion
  MapReduce single-handedly made big cluster computation popular.
  - Not the most efficient or flexible.
  + Scales well.
  + Easy to program -- failures and data movement are hidden.
  These were good trade-offs in practice.
  We'll see some more advanced successors later in the course.
  Have fun with the lab!
